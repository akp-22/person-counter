# person-counter
A product that detects and counts the number of people entering and exiting a store.

Localizing the objects of interest in the scene/image and classifying/labelling it are prime tasks for object recognition. A short overview of deep learning based open source algorithms is as follows.<br />

Region-Based Convolutional Neural Networks (R-CNN) [1] generate category independent region proposals for candidate bounding boxes and extract features using CNNs for feeding to classification model. Fast R-CNN uses a pre trained CNN on each ROI (Region of Interest) followed by a pooling layer and Fully Connected layer for class predictions and bounding boxes. Faster R-CNN uses a Region Proposal Network (RPN) followed by a CNN for feature extraction, bounding boxes and classification. RPN uses a pre-trained model such as VGG 16, ResNet 101, ResNet 50 and Inception. Mask R-CNN is an extended version of Faster R-CNN which also predicts a mask for each detected object. The drawbacks of these methods are too many region proposals make training expensive and slow during inference. And training also happens in multiple phases. R-CNN based Open Source pre-trained models on COCO dataset are available at [2].<br />

SingleShot MultiBox Detector (SSD) [3] does the object localization (by MultiBox regression technique) and classification in a single forward passes of the network. SSD uses fixed priors where every feature map cell is associated with a set of default bounding boxes of different dimensions and aspect ratios. SSD uses a base network such as VGG, Inception V2 and MobileNet by discarding the Fully Connected (FC) layers because of their strong performance in classification tasks and the popularity for problems where transfer learning helps in improving results. SSDs have good precision and speed compared to R-CNN based methods. The drawback of SSD being the shallow layers not generating enough high level features to do prediction for small objects. SSD based Open Source pre-trained models on COCO dataset are available at [2].<br />

RetinaNet solves the foreground-background class imbalance problem in a single stage detector. It is an SSD with Resnet as FPN feature extractor, shared box predictor and focal loss. The TensorFlow pre-trained model of RetinaNet on COCO dataset is available at [2] with the name as ssd_resnet_50_fpn_coco.pb<br />

You Only Look Once (YOLO) architecture [4] is a single stage detector where only one network simultaneously predicts multiple bounding boxes by posing it as a regression problem and also estimates true class probabilities for those boxes. YOLO works on the whole image during training as well as inference. It proves to be associating the contextual information about classes as well as their appearance. YOLO also suffers detecting objects which are smaller than the minimum grid size. YOLO is fast and can be made extremely faster (YOLO v3-tiny at 220 FPS at 31 mAP) compromising the precision. YOLO Open Source pre-trained models on COCO dataset are available at [5]. Some of the models trained on ImageNet are available at [6]<br />

Various benchmark (mAP) performances on different datasets (COCO, ISLVRC) report R-CNN runs at ~5 FPS, Mask R-CNN at 7-8 FPS, SSDs at 20-47 FPS, RetinaNet at 9-13 FPS and YOLO v3 at 20-45 FPS. For Real-time applications, generally speed of the network matters. If the Scene/image does not contain many small objects then I would recommend SSD/ RetinaNet first and then YOLO as the inferences on the not seen data are relatively better. If there are many small objects, then use Mask R-CNN. If the speed is not the criteria, Faster R-CNN is great. <br />

The approach I followed is as follows. <br />
1. Apply object detector to get bounding boxes for person class. <br />
2. If the detector is expensive to run on each frame for the given hardware, skip the detector and track the object for a certain number of frames. <br />
3. Associate and disassociate the tracked object as a unique object for counting. <br />

I have experimented with SSD as object detector with multiple versions of MobileNet, Inception and RetinaNet (SSD with ResNet as FPN) pre-trained models (on COCO dataset). I found RetinaNet detections are decent specifically for top view capturing camera setup. The number of skip frames of detector is highly dependent on the density of the people entering/exiting and the time window for counting. If the time window for the OOI (Object Of Interest) in the scene is high, we can take advantage of tracking for many skip frames of detection. In this assignment, I used tracker as the detector module is expensive for my computer configuration and the OOI active time in the video is good enough to count the persons. You can also see that if the tracker fails to give a bounding box along with the detector, the last tracked/detected bounding box appears for a specific number of frames till the detection module re-run. Keeping this number of frames higher makes it look the detections faulty. This has been done just to see the continuity of the tracked object. Since I used tracker in the place of detector for skip frames, I need good accuracy of results with decent speed. So I experimented with dlib correlation tracker and OpenCV KCF tracker. I found correlation tracker has slight edge over KCF due to optimized implementation in the open source toolbox. For counting each tracked person, a unique ID was issued. This association/disassociation logic of a specific ID for the tracked objects should be simpler. Andrian worked it out using the direction of distance between centroids of the bounding boxes in successive frames. So I used this logic from his blog [7]. The input video file is also taken from his blog. If the detection is not expensive for the given hardware setup, tracker can also run as a supplement which gives bounding boxes for the frames where detector module fails. I have written a script to experiment this but did not integrate in this code. It is available in the shared folder as exp_tracker.py file.
The count updating logic has to be slightly modified if the video frames/camera is not capturing top view. In general, the selection/modification of network architecture depends on the characteristics of the video data that we want inferences out of. Training, hyper parameter tuning, optimization and number of parameters have to be worked out.<br />

References: <br />
[1] Girshick, R., Donahue, J., Darrell, T., & Malik, J. (2014). Rich feature hierarchies for accurate object detection and semantic segmentation. In Proceedings of the IEEE CVPR (pp. 580-587).<br />
[2]https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/detection_model_zoo.md <br />
[3] Liu, Wei, et al. "Ssd: Single shot multibox detector." European conference on computer vision. Springer, Cham, 2016. [4] Redmon, J., Divvala, S., Girshick, R., & Farhadi, A. (2016). You only look once: Unified, real-time object detection. In Proceedings of the IEEE conference on computer vision and pattern recognition (pp. 779-788).<br />
[5] https://pjreddie.com/darknet/yolo/<br />
[6] https://github.com/onnx/models<br />
[7] https://www.pyimagesearch.com/2018/07/23/simple-object-tracking-with-opencv/
